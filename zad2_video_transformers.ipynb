{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMs8ahM2gu0Q"
      },
      "source": [
        "# Video Classification with Transformers\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/06/08<br>\n",
        "**Last modified:** 2023/22/07<br>\n",
        "**Description:** Training a video classifier with hybrid transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fn3yRAVgu0T"
      },
      "source": [
        "Ten przykład jest kontynuacją\n",
        "[Klasyfikacja wideo według architektury CNN-RNN](https://keras.io/examples/vision/video_classification/)\n",
        "przykład. Tym razem będziemy posługiwać się modelem opartym na Transformerze\n",
        "([Vaswani i in.](https://arxiv.org/abs/1706.03762)), aby klasyfikować filmy. Możesz śledzić\n",
        "[ten rozdział książki](https://livebook.manning.com/book/deep-learning-with-python-drugie-edition/chapter-11)\n",
        "na wypadek gdybyś potrzebował wprowadzenia do Transformers (z kodem). Po przeczytaniu tego\n",
        "na przykład będziesz wiedział, jak tworzyć hybrydowe modele oparte na transformatorach do zastosowań wideo\n",
        "klasyfikacja działająca na mapach obiektowych CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igS9ZWlwgu0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5c15c0-9968-4860-c612-4a3ca9fb7d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDJiW_VFgu0U"
      },
      "source": [
        "## Zbieranie danych\n",
        "\n",
        "Podobnie jak w [poprzedniku](https://keras.io/examples/vision/video_classification/)\n",
        "w tym przykładzie użyjemy podpróbkowanej wersji pliku\n",
        "[Zbiór danych UCF101](https://www.crcv.ucf.edu/data/UCF101.php),\n",
        "dobrze znany zbiór danych porównawczych. Jeśli chcesz operować na większej podpróbce lub\n",
        "nawet cały zbiór danych, proszę zapoznać się z\n",
        "[ten notatnik](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycIs5rhUgu0V"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n",
        "!tar -xf ucf101_top5.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbXzrbL7gu0V"
      },
      "source": [
        "## Konfiguracja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "id": "5cMJhbM8gxlw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebaa561-1cf4-47d9-ac97-4c55858830a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBbxvTo8gu0V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications.densenet import DenseNet121\n",
        "\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QCYHVuugu0W"
      },
      "source": [
        "## Zdefiniuj hiperparametry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXw3124Ygu0W"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ta1ch3Jgu0W"
      },
      "source": [
        "## Przygotowywanie danych\n",
        "\n",
        "W tym przykładzie będziemy głównie wykonywać te same kroki przygotowania danych, z wyjątkiem\n",
        "następujące zmiany:\n",
        "\n",
        "* Zmniejszamy rozmiar obrazu do 128x128 zamiast 224x224, aby przyspieszyć obliczenia.\n",
        "* Zamiast używać wstępnie wyszkolonej sieci [InceptionV3](https://arxiv.org/abs/1512.00567),\n",
        "używamy wstępnie przeszkolonego\n",
        "[DenseNet121](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n",
        "do ekstrakcji cech.\n",
        "* Bezpośrednio dopasowujemy krótsze filmy do długości `MAX_SEQ_LENGTH`.\n",
        "\n",
        "Najpierw załadujmy plik\n",
        "[DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQjc6YqXgu0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a90b4a9-e701-475e-b7c1-e13613eaac31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 594\n",
            "Total videos for testing: 224\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = keras.ops.convert_to_numpy(cropped)\n",
        "    cropped = keras.ops.squeeze(cropped)\n",
        "    return cropped\n",
        "\n",
        "def load_video(path, max_frames=0, offload_to_cpu=False):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frame = crop_center(frame)\n",
        "            if offload_to_cpu and keras.backend.backend() == \"torch\":\n",
        "                frame = frame.to(\"cpu\")\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    if offload_to_cpu and keras.backend.backend() == \"torch\":\n",
        "        return np.array([frame.to(\"cpu\").numpy() for frame in frames])\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = DenseNet121(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for idx, path in enumerate(video_paths):\n",
        "\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKIDW-s3gu0X"
      },
      "source": [
        "Wywołanie funkcji `prepare_all_videos()` na `train_df` i `test_df` zajmuje około 20 minut\n",
        "kompletny. Z tego powodu, aby zaoszczędzić czas, tutaj pobieramy już wstępnie przetworzone tablice NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyJYJ9Augu0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06167b23-5eca-41d2-f3ff-b08391a87184"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "!!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!!tar -xf top5_data_prepared.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkhVfcNRgu0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af8aa73-b63a-4ec7-e1f3-f1df860f17bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (594, 20, 1024)\n"
          ]
        }
      ],
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBmC38kCgu0X"
      },
      "source": [
        "## Budowa modelu opartego na transformatorze\n",
        "\n",
        "Będziemy budować na podstawie udostępnionego kodu\n",
        "[ten rozdział książki](https://livebook.manning.com/book/deep-learning-with-python-drugie-edition/chapter-11)\n",
        "[Głębokie uczenie się z Pythonem (wyd. drugie)](https://www.manning.com/books/deep-learning-with-python)\n",
        "przez François Cholleta.\n",
        "\n",
        "Po pierwsze, są to warstwy samouważności, które tworzą podstawowe bloki Transformatora\n",
        "niezależny od porządku. Ponieważ filmy są uporządkowanymi sekwencjami klatek, potrzebujemy naszego\n",
        "Model transformatora uwzględniający informacje o zamówieniu.\n",
        "Robimy to poprzez **kodowanie pozycyjne**.\n",
        "Po prostu osadzamy pozycje klatek znajdujących się w filmach za pomocą pliku\n",
        "[Warstwa „Osadzanie”](https://keras.io/api/layers/core_layers/embedding). Wtedy my\n",
        "dodaj te osadzania pozycyjne do wstępnie obliczonych map obiektów CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhEKfCeygu0X"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.position_embeddings.build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = keras.ops.cast(inputs, self.compute_dtype)\n",
        "        length = keras.ops.shape(inputs)[1]\n",
        "        positions = keras.ops.arange(start=0, stop=length, step=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkHrhZOlgu0Y"
      },
      "source": [
        "Teraz możemy utworzyć warstwę podklasy dla Transformera."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQbrn-Ekgu0Y"
      },
      "source": [
        "## Funkcje użytkowe do treningu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXDoaWwOgu0Y"
      },
      "source": [
        "## Trenowanie i wnioskowanie modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIZz-sFIgu0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9300c6e-10a1-4667-f22c-f91bd6ef1ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5706 - loss: 2.1843\n",
            "Epoch 1: val_loss improved from inf to 1.64706, saving model to /tmp/video_classifier.weights.h5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.5799 - loss: 2.1322 - val_accuracy: 0.4444 - val_loss: 1.6471\n",
            "Epoch 2/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9541 - loss: 0.1710\n",
            "Epoch 2: val_loss improved from 1.64706 to 0.34001, saving model to /tmp/video_classifier.weights.h5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.9546 - loss: 0.1686 - val_accuracy: 0.9000 - val_loss: 0.3400\n",
            "Epoch 3/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9955 - loss: 0.0153\n",
            "Epoch 3: val_loss did not improve from 0.34001\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9954 - loss: 0.0152 - val_accuracy: 0.6222 - val_loss: 1.3465\n",
            "Epoch 4/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9907 - loss: 0.0179\n",
            "Epoch 4: val_loss improved from 0.34001 to 0.29268, saving model to /tmp/video_classifier.weights.h5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9909 - loss: 0.0176 - val_accuracy: 0.9222 - val_loss: 0.2927\n",
            "Epoch 5/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0078\n",
            "Epoch 5: val_loss improved from 0.29268 to 0.24385, saving model to /tmp/video_classifier.weights.h5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0079 - val_accuracy: 0.9556 - val_loss: 0.2439\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy: 0.8280 - loss: 1.1518\n",
            "Test accuracy: 85.27%\n"
          ]
        }
      ],
      "source": [
        "trained_model = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZcjT1Sggu0Y"
      },
      "source": [
        "**Uwaga**: Ten model ma ~4,23 miliona parametrów, czyli znacznie więcej niż sekwencja\n",
        "model (99918 parametrów), którego użyliśmy w prequelu tego przykładu. Ten rodzaj\n",
        "Model transformatora działa najlepiej w przypadku większego zbioru danych i dłuższego harmonogramu przedtreningowego."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F0A45pxgu0Y"
      },
      "outputs": [],
      "source": [
        "def prepare_single_video(frames):\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    if len(frames) < MAX_SEQ_LENGTH:\n",
        "        diff = MAX_SEQ_LENGTH - len(frames)\n",
        "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "        frames = np.concatenate(frames, padding)\n",
        "\n",
        "    frames = frames[None, ...]\n",
        "\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            if np.mean(batch[j, :]) > 0.0:\n",
        "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "            else:\n",
        "                frame_features[i, j, :] = 0.0\n",
        "\n",
        "    return frame_features\n",
        "\n",
        "\n",
        "def predict_action(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(os.path.join(\"test\", path), offload_to_cpu=True)\n",
        "    frame_features = prepare_single_video(frames)\n",
        "    probabilities = trained_model.predict(frame_features)[0]\n",
        "\n",
        "    plot_x_axis, plot_y_axis = [], []\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        plot_x_axis.append(class_vocab[i])\n",
        "        plot_y_axis.append(probabilities[i])\n",
        "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "\n",
        "    plt.bar(plot_x_axis, plot_y_axis, label=plot_x_axis)\n",
        "    plt.xlabel(\"class_label\")\n",
        "    plt.xlabel(\"Probability\")\n",
        "    plt.show()\n",
        "\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZrOrLnpgu0Z"
      },
      "source": [
        "Wydajność naszego modelu jest daleka od optymalnej, ponieważ został wytrenowany na platformie\n",
        "mały zbiór danych."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}